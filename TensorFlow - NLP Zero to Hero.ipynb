{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18a064cb",
   "metadata": {},
   "source": [
    "# Lesson 1 - Tokenization\n",
    "\n",
    "- Process of encoding sentences into numbers <br>\n",
    "- Words are represented as numbers for computers to process them <br>\n",
    "- \"LISTEN\" and \"SILENT\" have the same letters with different orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50647ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfe0785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93bcadd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af1ab2d",
   "metadata": {},
   "source": [
    "# Lesson 2 - Sequencing\n",
    "\n",
    "- Creating sequences of numbers from the sentences and using tools to process these sequences of numbers to train the neural networks <br>\n",
    "- If the testing data of sentences contain new words that were not found in the training data, these new words would not be represented based on the training corpus. Hence, losing the words and length of the sequence <br>\n",
    "- One method of overcoming this problem of losing the words and length of sequence due to not having the new words in the training corpus is to set the property of the Tokenizer(oov_token = \"\\<OOV\\>\") <br>\n",
    "- This allows the Tokenizer to create a Out-Of-Vocabulary (OOV) Token and replacing the words it does not recognise with the OOV Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6948dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "482a35fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n"
     ]
    }
   ],
   "source": [
    "# encode words into tokens and index these tokens\n",
    "tokenizer = Tokenizer(num_words=100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66578931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
     ]
    }
   ],
   "source": [
    "# generates sequences of tokens that represent the sentences\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5d22274",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    'i really love my dog',\n",
    "    'my dog loves my manatee'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fb428dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 2, 1, 3], [1, 3, 1]]\n"
     ]
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5449857b",
   "metadata": {},
   "source": [
    "#### With Out-Of-Vocabulary (OOV) Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dccae205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
     ]
    }
   ],
   "source": [
    "# encode words into tokens and index these tokens\n",
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c61a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    'i really love my dog',\n",
    "    'my dog loves my manatee'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ff7cf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3adc926",
   "metadata": {},
   "source": [
    "- For training of a neural network, the length of the sentences have to be the same <br>\n",
    "- The advanced method to handle sentences of different lengths is to use RaggedTensor: https://www.tensorflow.org/api_docs/python/tf/RaggedTensor \n",
    "- Simpler solution is to use **PADDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a37bf8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59a18dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01935b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
     ]
    }
   ],
   "source": [
    "# encode words into tokens and index these tokens\n",
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d463237f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fe6f368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  5  3  2  4]\n",
      " [ 0  0  0  5  3  2  7]\n",
      " [ 0  0  0  6  3  2  4]\n",
      " [ 8  6  9  2  4 10 11]]\n"
     ]
    }
   ],
   "source": [
    "pre_padded = pad_sequences(sequences)\n",
    "\n",
    "print(pre_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "368ff9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  3  2  4  0  0  0]\n",
      " [ 5  3  2  7  0  0  0]\n",
      " [ 6  3  2  4  0  0  0]\n",
      " [ 8  6  9  2  4 10 11]]\n"
     ]
    }
   ],
   "source": [
    "# paddings after the sequence\n",
    "post_padded = pad_sequences(sequences, padding='post')\n",
    "\n",
    "print(post_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c432c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  3  2  4  0  0]\n",
      " [ 5  3  2  7  0  0]\n",
      " [ 6  3  2  4  0  0]\n",
      " [ 6  9  2  4 10 11]]\n"
     ]
    }
   ],
   "source": [
    "# limit the length of the sequence + padding\n",
    "post_padded_maxlen = pad_sequences(sequences, padding='post', maxlen=6)\n",
    "\n",
    "print(post_padded_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1950809b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 3 2 4 0]\n",
      " [5 3 2 7 0]\n",
      " [6 3 2 4 0]\n",
      " [8 6 9 2 4]]\n"
     ]
    }
   ],
   "source": [
    "# truncating if the length of sequence exceeds the max_length\n",
    "post_padded_trunc_maxlen = pad_sequences(sequences, padding='post',\n",
    "                                   truncating='post', maxlen=5)\n",
    "\n",
    "print(post_padded_trunc_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eff7a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
